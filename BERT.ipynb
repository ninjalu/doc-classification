{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with BERT\n",
    "Before diving into the details: the training and validation were not performed from this notebook but a AWS EC2 instance. If you want to replicate the result, please run BERT.py on a machine with GPU. </n>\n",
    "\n",
    "After playing around with traditional NLP methods, I think  them not being able to capturing the meaning of words and styles of writing may have a big impact on the accuracy. Transformers have a deeper understanding of words and sequences beyond frequencies. In attempt to improve accuracy, I propose the following strategies to try. Each may have variances within and trying to solve different challenges of BERT. </n>\n",
    "\n",
    "1. **BERT for sequence classification model with pretrained weights and embeddings:** </n>\n",
    "- Simple truncate: truncate the document head only, tail only, and head and tail; Hierachical method: divide the document into L/510 fragments and then use mean pooling, max pooling and self attention to combine hidden states of [CLS] for each fragment. There is [suggestion](https://arxiv.org/pdf/1905.05583.pdf) that head and tail produces better results on certain corpus than hierachical methods. </n>\n",
    "- [Longformer](https://arxiv.org/pdf/2004.05150.pdf): A transformer that handles long documents. [Code](https://github.com/allenai/longformer)\n",
    "\n",
    "2. **Fine tune BERT MLM on a domain specific corpus and use the updated weights (and embeddings) for BERT sequence classification**\n",
    "\n",
    "3. **Ensemble of traditional NLP with BERT.**\n",
    "\n",
    "4. **Knowledge distillation** \n",
    "5. **Make more data dividing documents into segments, each labelled accordingly.\n",
    "6. **Back translation to create more data**\n",
    "\n",
    "Options 1 and 4, 5 are trying to address the problem of long sequence, 2 lack of domain specific training, and 3 tackling the problem from different angles (tranditional NLP: frequencies, BERT: word meaning and style). Option 6 is trying to tackle the problem of small training set. </n>\n",
    "\n",
    "Because of time and resource contraint we are going to try combining 1 with head truncation, 2 and 6. It's possible that traditional NLP and neural NLP could provide insights from different aspects and it would be interesting to explore here. Given time and resouce, other ideas would be good to explore too!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.a Vanilla BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import BertEncoder, build_dataloaders\n",
    "from fine_tune_bert import fine_tune_bert\n",
    "import pickle5 as pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    label = df['class_id'].apply(lambda x: int(x))\n",
    "    \n",
    "    texts, _, label, _ = train_test_split(texts, label, test_size=0.2, stratify=label, random_state=2020)\n",
    "    dataset = BertEncoder(\n",
    "        tokenizer=BertTokenizer.from_pretrained(\n",
    "            'bert-base-cased', \n",
    "            do_lower_case=False), \n",
    "        input_data=texts\n",
    "    )\n",
    "\n",
    "    data = dataset.tokenize(max_len=510)\n",
    "    input_ids, attention_masks = data\n",
    "    label = torch.Tensor(label.to_list()).long()\n",
    "    train_dataloader, val_dataloader = build_dataloaders(\n",
    "        input_ids=input_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        labels=label,\n",
    "        batch_size=(16, 16),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    trained_model, stats = fine_tune_bert(\n",
    "        train_dataloader=train_dataloader, \n",
    "        valid_dataloader=val_dataloader, \n",
    "        model=bert_model,\n",
    "        optimizer=optimizer,\n",
    "        save_model_path='model/trained_model.pt',\n",
    "        save_stats_dict_path='logs/statistics.json',\n",
    "        device = device,\n",
    "        epochs = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 21 epochs of training, before it was stopped because validation accuracy had not changed for a few epochs: </n>\n",
    "![Screenshot](training_result.png) </n>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is saved and ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.77      0.94      0.85        18\n",
      " Situation Report       0.94      0.85      0.89        20\n",
      "   Profile report       1.00      0.65      0.79        20\n",
      "Analytical report       0.75      1.00      0.86        15\n",
      "\n",
      "         accuracy                           0.85        73\n",
      "        macro avg       0.87      0.86      0.85        73\n",
      "     weighted avg       0.88      0.85      0.85        73\n",
      "\n",
      "[[17  1  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 4  0 13  3]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('tokenised-data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=BertTokenizer.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        do_lower_case=False),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "bert_state_dict = torch.load('./model/bfs_trained_model.pt')\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    state_dict=bert_state_dict,\n",
    "    num_labels = 4,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "y_b_pred, _ = bert_predict(bert, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.b RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drawback of vanilla BERT model: the tokeniser and pre-trained BERT model were trained on Wikipedia and BookCorpus. Although it might have a good 'understanding' of Wiki and book English, it has not seen news and analytical reports that are closer to our training data. </n>\n",
    "Here enters [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). The advantage of RoBERTa here are two. First, it was pre-trained on Wikipedia, BookCorpus, plus CC-NEWS, OpenWebText and STORIES, the later three are all texts from the internet, much closer to the kind of texts we see in our corpus. Two, RoBERTa also employs different training strategies, arguably more effective for downstream tasks. </n>\n",
    "However, we managed to achieve no loss in the first epoch (not sure it is a good thing or not?). Either out training data is too small, or RoBERTa is too great?</n>\n",
    "I evaluate both BERT model and RoBERTa model using the same testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not run on CPU.\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import *\n",
    "from fine_tune_bert_sc import *\n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['class_id'].apply(lambda x: int(x))\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=2020)\n",
    "\n",
    "    train_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=train_texts\n",
    "    )\n",
    "    test_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=test_texts\n",
    "    )\n",
    "\n",
    "    train_data = train_dataset.tokenize(max_len=510)\n",
    "    test_data = test_dataset.tokenize(max_len=510)\n",
    "\n",
    "    train_input_ids, train_attention_masks = train_data\n",
    "    test_input_ids, test_attention_masks = test_data\n",
    "\n",
    "    train_labels = torch.Tensor(train_labels.to_list()).long()\n",
    "    test_labels = torch.Tensor(test_labels.to_list()).long()\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='.results/',\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=dummy_data_collector\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model('.model/')\n",
    "    trainer.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.95      1.00      0.97        18\n",
      " Situation Report       1.00      0.85      0.92        20\n",
      "   Profile report       1.00      1.00      1.00        20\n",
      "Analytical report       0.88      1.00      0.94        15\n",
      "\n",
      "         accuracy                           0.96        73\n",
      "        macro avg       0.96      0.96      0.96        73\n",
      "     weighted avg       0.96      0.96      0.96        73\n",
      "\n",
      "[[18  0  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "roberta = RobertaForSequenceClassification.from_pretrained(\n",
    "    'model/roberta',\n",
    "    local_files_only=True\n",
    ")\n",
    "y_b_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It look like RoBERTa works a lot better than vanilla BERT model. Notice the model only misclassified 3 situation reports in test set. Let's run the model on the full data (train and test) to see which ones are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.97      1.00      0.98        88\n",
      " Situation Report       1.00      0.95      0.97        99\n",
      "   Profile report       1.00      1.00      1.00       100\n",
      "Analytical report       0.96      0.99      0.97        74\n",
      "\n",
      "         accuracy                           0.98       361\n",
      "        macro avg       0.98      0.98      0.98       361\n",
      "     weighted avg       0.98      0.98      0.98       361\n",
      "\n",
      "[[ 88   0   0   0]\n",
      " [  2  94   0   3]\n",
      " [  0   0 100   0]\n",
      " [  1   0   0  73]]\n"
     ]
    }
   ],
   "source": [
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=X.to_list()\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(y.apply(lambda x:int(x)).to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "y_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(y_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model misclassified ground truth 26     1\n",
      "68     1\n",
      "75     1\n",
      "77     1\n",
      "336    1\n",
      "347    3\n",
      "Name: class_id, dtype: int64\n",
      "BERT model misclassified predicted [0 0 3 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "print('BERT model misclassified ground truth', y[y!=y_pred])\n",
    "print('BERT model misclassified predicted', np.array(y_pred)[y!=y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified cases are: </n>\n",
    "\n",
    "26 and 86 ground truth: situation report, prediction: incident report </n>\n",
    "\n",
    "74, 77 and 336 ground truth: situation report, prediction: profile report </n>\n",
    "\n",
    "347 ground truth: profile report, prediction: incident report </n>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Finetune the BERT model with a MLM task and finetune again with classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind this is the general idea pretrained models improve performance on downstream task. Because BERT might not have seen the kind of training data here, fine tuning BERT weights with the texts similar to the training data would be benefitial. </n>\n",
    "\n",
    "Because labelled data is hard to come by, BERT Masked Language Model would be very useful. </n>\n",
    "\n",
    "The original proposal is to obtain more documents of the similar kind, however, we weren't able to because of the time constraint. I could fine tune BERT with our training data on hand, however, I have little confidence it would work well because: first, less than 400 data points is too small to fine tune BERT weights effectively; second, if the model has seen the training data, would it constitute data leakage? </n>\n",
    "\n",
    "Maybe this is a good avenue to explore given additional data (how much?) </n>\n",
    "\n",
    "Given how well RoBERTa performs, I think it's fair to say any improvement on it with further MLM fine tuning would be quite amazing and proves how adaptable BERT is. The implication is not only for this classification task, but all downstream task such as entity recognition. Perhaps unsupervised learning techniques could bring overall improvements on all tasks using similar kind of data. To further increase the power of the fine tuning, perhaps we could even enlist many NLP augmentation methods to bring the power of fine tuning even futher. Endless possibilities!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Backtranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, I utilise [nlpaug](https://github.com/makcedward/nlpaug) -- a NLP augmentation API with a backgtranslation class that uses [pointers to the models of Facebook-FAIR's WMT'19 news translation task submission](https://github.com/pytorch/fairseq/tree/master/examples/wmt19) <br>\n",
    "\n",
    "The fair wmt19 model contrains pretrained model for two languages, German and Russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import pandas as pd\n",
    "import pickle\n",
    "def back_translation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, backtranslate the text column and append to the original dataframe with labels \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing text and labels\n",
    "        col_name (str): Name of the text column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing original text and backtranslated text and labels\n",
    "    \"\"\"\n",
    "\n",
    "    MODELS = {\n",
    "        'GERMAN': ('transformer.wmt19.en-de', 'transformer.wmt19.de-en'),\n",
    "        'RUSSIAN': ('transformer.wmt19.en-ru', 'transformer.wmt19.ru-en')\n",
    "    }\n",
    "\n",
    "    for _, model in MODELS.items():\n",
    "        back_translation_aug = naw.BackTranslationAug(\n",
    "            from_model_name=model[0],\n",
    "            to_model_name=model[1]\n",
    "        )\n",
    "        translation = df['col_name'].apply(back_translation_aug.augment)\n",
    "        labels = df['class_id']\n",
    "        trans_df = pd.DataFrame({\n",
    "            'text': translation,\n",
    "            'class_id': labels\n",
    "        })\n",
    "\n",
    "        df = pd.concat([df, trans_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      " 69%|██████▉   | 8223920128/11958904958 [3:48:23<1:43:43, 600135.83B/s]\n",
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      "100%|██████████| 11958904958/11958904958 [15:37<00:00, 12750993.89B/s]\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumped over the lazy dog'\n",
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='transformer.wmt19.en-de',\n",
    "    to_model_name='transformer.wmt19.de-en')\n",
    "back_translation_aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      " 80%|████████  | 9584486400/11946275315 [36:21<03:08, 12557855.58B/s] "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-68c79641fe1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c4430a5e25c7>\u001b[0m in \u001b[0;36mback_translation\u001b[0;34m(df, col_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         back_translation_aug = naw.BackTranslationAug(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mfrom_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mto_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/augmenter/word/back_translation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, from_model_name, to_model_name, from_model_checkpt, to_model_checkpt, tokenizer, bpe, is_load_from_github, name, device, force_reload, verbose)\u001b[0m\n\u001b[1;32m     65\u001b[0m             device=device, verbose=verbose, include_detail=False, parallelable=True)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         self.model = self.get_model(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mfrom_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_model_checkpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_model_checkpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mto_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_model_checkpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_model_checkpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/augmenter/word/back_translation.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(cls, from_model_name, from_model_checkpt, to_model_name, to_model_checkpt, tokenzier_name, bpe_name, device, is_load_from_github, force_reload)\u001b[0m\n\u001b[1;32m     84\u001b[0m     def get_model(cls, from_model_name, from_model_checkpt, to_model_name, to_model_checkpt, \n\u001b[1;32m     85\u001b[0m             tokenzier_name, bpe_name, device='cuda', is_load_from_github=True, force_reload=False):\n\u001b[0;32m---> 86\u001b[0;31m         return init_back_translatoin_model(from_model_name, from_model_checkpt, \n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mto_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_model_checkpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenzier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpe_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mis_load_from_github\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/augmenter/word/back_translation.py\u001b[0m in \u001b[0;36minit_back_translatoin_model\u001b[0;34m(from_model_name, from_model_checkpt, to_model_name, to_model_checkpt, tokenzier_name, bpe_name, is_load_from_github, device, force_reload)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBACK_TRANSLATION_MODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     model = nml.Fairseq(from_model_name=from_model_name, from_model_checkpt=from_model_checkpt, \n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mto_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_model_checkpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_model_checkpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtokenzier_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenzier_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpe_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpe_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_load_from_github\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_load_from_github\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/model/lang_models/fairseq.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, from_model_name, from_model_checkpt, to_model_name, to_model_checkpt, is_load_from_github, tokenzier_name, bpe_name, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_load_from_github\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             self.from_model = torch.hub.load(\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;34m'pytorch/fairseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_model_checkpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/models/fairseq_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhub_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         x = hub_utils.from_pretrained(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# convenience hack for loading data and BPE codes from model archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_name_or_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_archive_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "with open('data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "df = back_translation(df, 'text')\n",
    "        back_translation_aug = naw.BackTranslationAug(\n",
    "            from_model_name=model[0],\n",
    "            to_model_name=model[1]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
