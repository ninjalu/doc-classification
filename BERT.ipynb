{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with BERT\n",
    "Before diving into the details: the training and validation were not performed from this notebook but a AWS EC2 instance. If you want to replicate the result, please run BERT.py on a machine with GPU. </n>\n",
    "\n",
    "After playing around with traditional NLP methods, I think  them not being able to capturing the meaning of words and styles of writing may have a big impact on the accuracy. Transformers have a deeper understanding of words and sequences beyond frequencies. In attempt to improve accuracy, I propose the following strategies to try. Each may have variances within and trying to solve different challenges of BERT. </n>\n",
    "\n",
    "1. **BERT for sequence classification model with pretrained weights and embeddings:** </n>\n",
    "- Simple truncate: truncate the document head only, tail only, and head and tail; Hierachical method: divide the document into L/510 fragments and then use mean pooling, max pooling and self attention to combine hidden states of [CLS] for each fragment. There is [suggestion](https://arxiv.org/pdf/1905.05583.pdf) that head and tail produces better results on certain corpus than hierachical methods. </n>\n",
    "- [Longformer](https://arxiv.org/pdf/2004.05150.pdf): A transformer that handles long documents. [Code](https://github.com/allenai/longformer)\n",
    "\n",
    "2. **Fine tune BERT MLM on a domain specific corpus and use the updated weights (and embeddings) for BERT sequence classification**\n",
    "\n",
    "3. **Ensemble of traditional NLP with BERT.**\n",
    "\n",
    "4. **Knowledge distillation** \n",
    "5. **Make more data dividing documents into segments, each labelled accordingly.**\n",
    "6. **Back translation to create more data**\n",
    "\n",
    "Options 1 and 4, 5 are trying to address the problem of long sequence, 2 lack of domain specific training, and 3 tackling the problem from different angles (tranditional NLP: frequencies, BERT: word meaning and style). Option 6 is trying to tackle the problem of small training set. </n>\n",
    "\n",
    "Because of time and resource contraint I have tried combining 1 with head truncation, 2, 5 and 6 with varied sucess. It's possible that traditional NLP and neural NLP could provide insights from different aspects and it would be interesting to explore here. Given time and resouce, other ideas would be good to explore too!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.a Vanilla BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import BertEncoder, build_dataloaders\n",
    "from fine_tune_bert import fine_tune_bert\n",
    "import pickle5 as pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    label = df['class_id'].apply(lambda x: int(x))\n",
    "    \n",
    "    texts, _, label, _ = train_test_split(texts, label, test_size=0.2, stratify=label, random_state=2020)\n",
    "    dataset = BertEncoder(\n",
    "        tokenizer=BertTokenizer.from_pretrained(\n",
    "            'bert-base-cased', \n",
    "            do_lower_case=False), \n",
    "        input_data=texts\n",
    "    )\n",
    "\n",
    "    data = dataset.tokenize(max_len=510)\n",
    "    input_ids, attention_masks = data\n",
    "    label = torch.Tensor(label.to_list()).long()\n",
    "    train_dataloader, val_dataloader = build_dataloaders(\n",
    "        input_ids=input_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        labels=label,\n",
    "        batch_size=(16, 16),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    trained_model, stats = fine_tune_bert(\n",
    "        train_dataloader=train_dataloader, \n",
    "        valid_dataloader=val_dataloader, \n",
    "        model=bert_model,\n",
    "        optimizer=optimizer,\n",
    "        save_model_path='model/trained_model.pt',\n",
    "        save_stats_dict_path='logs/statistics.json',\n",
    "        device = device,\n",
    "        epochs = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 21 epochs of training, before it was stopped because validation accuracy had not changed for a few epochs: </n>\n",
    "![Screenshot](training_result.png) </n>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is saved and ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.77      0.94      0.85        18\n",
      " Situation Report       0.94      0.85      0.89        20\n",
      "   Profile report       1.00      0.65      0.79        20\n",
      "Analytical report       0.75      1.00      0.86        15\n",
      "\n",
      "         accuracy                           0.85        73\n",
      "        macro avg       0.87      0.86      0.85        73\n",
      "     weighted avg       0.88      0.85      0.85        73\n",
      "\n",
      "[[17  1  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 4  0 13  3]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('tokenised-data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=BertTokenizer.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        do_lower_case=False),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "bert_state_dict = torch.load('./model/bfs_trained_model.pt')\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    state_dict=bert_state_dict,\n",
    "    num_labels = 4,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "y_b_pred, _ = bert_predict(bert, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.b RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drawback of vanilla BERT model: the tokeniser and pre-trained BERT model were trained on Wikipedia and BookCorpus. Although it might have a good 'understanding' of Wiki and book English, it has not seen news and analytical reports that are closer to our training data. </n>\n",
    "Here enters [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). The advantage of RoBERTa here are two. First, it was pre-trained on Wikipedia, BookCorpus, plus CC-NEWS, OpenWebText and STORIES, the later three are all texts from the internet, much closer to the kind of texts we see in our corpus. Two, RoBERTa also employs different training strategies, arguably more effective for downstream tasks. </n>\n",
    "However, we managed to achieve no loss in the first epoch (not sure it is a good thing or not?). Either out training data is too small, or RoBERTa is too great?</n>\n",
    "I evaluate both BERT model and RoBERTa model using the same testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not run on CPU.\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import *\n",
    "from fine_tune_bert_sc import *\n",
    "import pickle5 as pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['class_id'].apply(lambda x: int(x))\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=2020)\n",
    "\n",
    "    train_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=train_texts\n",
    "    )\n",
    "    test_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=test_texts\n",
    "    )\n",
    "\n",
    "    train_data = train_dataset.tokenize(max_len=510)\n",
    "    test_data = test_dataset.tokenize(max_len=510)\n",
    "\n",
    "    train_input_ids, train_attention_masks = train_data\n",
    "    test_input_ids, test_attention_masks = test_data\n",
    "\n",
    "    train_labels = torch.Tensor(train_labels.to_list()).long()\n",
    "    test_labels = torch.Tensor(test_labels.to_list()).long()\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='.results/',\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=dummy_data_collector\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model('.model/')\n",
    "    trainer.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.95      1.00      0.97        18\n",
      " Situation Report       1.00      0.85      0.92        20\n",
      "   Profile report       1.00      1.00      1.00        20\n",
      "Analytical report       0.88      1.00      0.94        15\n",
      "\n",
      "         accuracy                           0.96        73\n",
      "        macro avg       0.96      0.96      0.96        73\n",
      "     weighted avg       0.96      0.96      0.96        73\n",
      "\n",
      "[[18  0  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "roberta = RobertaForSequenceClassification.from_pretrained(\n",
    "    'model/roberta',\n",
    "    local_files_only=True\n",
    ")\n",
    "y_b_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It look like RoBERTa works a lot better than vanilla BERT model. Notice the model only misclassified 3 situation reports in test set. Let's run the model on the full data (train and test) to see which ones are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.97      1.00      0.98        88\n",
      " Situation Report       1.00      0.95      0.97        99\n",
      "   Profile report       1.00      1.00      1.00       100\n",
      "Analytical report       0.96      0.99      0.97        74\n",
      "\n",
      "         accuracy                           0.98       361\n",
      "        macro avg       0.98      0.98      0.98       361\n",
      "     weighted avg       0.98      0.98      0.98       361\n",
      "\n",
      "[[ 88   0   0   0]\n",
      " [  2  94   0   3]\n",
      " [  0   0 100   0]\n",
      " [  1   0   0  73]]\n"
     ]
    }
   ],
   "source": [
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=X.to_list()\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(y.apply(lambda x:int(x)).to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "y_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(y_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model misclassified ground truth 26     1\n",
      "68     1\n",
      "75     1\n",
      "77     1\n",
      "336    1\n",
      "347    3\n",
      "Name: class_id, dtype: int64\n",
      "BERT model misclassified predicted [0 0 3 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "print('BERT model misclassified ground truth', y[y!=y_pred])\n",
    "print('BERT model misclassified predicted', np.array(y_pred)[y!=y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified cases are: </n>\n",
    "\n",
    "26 and 86 ground truth: situation report, prediction: incident report </n>\n",
    "\n",
    "74, 77 and 336 ground truth: situation report, prediction: profile report </n>\n",
    "\n",
    "347 ground truth: profile report, prediction: incident report </n>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Finetune the BERT model with a MLM task and finetune again with classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind this is the general idea pretrained models improve performance on downstream task. Because BERT might not have seen the kind of training data here, fine tuning BERT weights with the texts similar to the training data would be benefitial. </n>\n",
    "\n",
    "Because labelled data is hard to come by, BERT Masked Language Model would be very useful. </n>\n",
    "\n",
    "The original proposal is to obtain more documents of the similar kind, however, we weren't able to because of the time constraint. I could fine tune BERT with our training data on hand, however, I have little confidence it would work well because: first, less than 400 data points is too small to fine tune BERT weights effectively; second, if the model has seen the training data, would it constitute data leakage? </n>\n",
    "\n",
    "Maybe this is a good avenue to explore given additional data (how much?) </n>\n",
    "\n",
    "Given how well RoBERTa performs, I think it's fair to say any improvement on it with further MLM fine tuning would be quite amazing and proves how adaptable BERT is. The idea is not only for this classification task, but all downstream tasks such as entity recognition. It is probable that semi-supervised learning techniques would bring overall improvements on all tasks using similar kind of data. </n>\n",
    "\n",
    "This has a very exciting practical implication. Instead of updating the model with labelled data (expensive and not easy to get), update BERT weights with all new data as it comes in. Like making a master sour dough, the BERT weights will get better and better and evolve with change (new vocabulary, changes in document types) </n>\n",
    "\n",
    "Traditional NLP although also very effective and much quicker, will have to rely on retraining on labelled data to maintain. In addition, it will not be able to help down stream tasks such as entity recognition, sentiment analysis etc. Finally, even though training and inference takes very little time, particularly for randam forest, the time to tokenize is not particularly short if you have a large amount of data. Although, NN NLP has the same problem as well. </n>\n",
    "\n",
    "In summary, as the original [RoBERTa paper](https://arxiv.org/pdf/1907.11692.pdf) pointed out, it achieved state-of-the-art performance from bigger batches, longer sequences, longer training time and more data! I think it points to the fact that ceiling probably has not been achieved and further training and fine tuning will give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Backtranslate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, I utilise [nlpaug](https://github.com/makcedward/nlpaug) -- a NLP augmentation API with a backgtranslation class that uses [pointers to the models of Facebook-FAIR's WMT'19 news translation task submission](https://github.com/pytorch/fairseq/tree/master/examples/wmt19) <br>\n",
    "\n",
    "The fair wmt19 model contrains pretrained model for two languages, German and Russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      "100%|██████████| 13605842351/13605842351 [17:31<00:00, 12939043.29B/s]\n",
      "Error when composing. Overrides: ['common.no_progress_bar=True', 'common.log_interval=100', \"common.log_format='simple'\", 'common.tensorboard_logdir=null', 'common.wandb_project=null', 'common.azureml_logging=False', 'common.seed=1', 'common.cpu=False', 'common.tpu=False', 'common.bf16=False', 'common.memory_efficient_bf16=False', 'common.fp16=True', 'common.memory_efficient_fp16=False', 'common.fp16_no_flatten_grads=False', 'common.fp16_init_scale=128', 'common.fp16_scale_window=null', 'common.fp16_scale_tolerance=0.0', 'common.min_loss_scale=0.0001', 'common.threshold_loss_scale=null', 'common.user_dir=null', 'common.empty_cache_freq=0', 'common.all_gather_list_size=16384', 'common.model_parallel_size=1', 'common.quantization_config_path=null', 'common.profile=False', 'common.reset_logging=True', 'common_eval.path=null', 'common_eval.post_process=null', 'common_eval.quiet=False', \"common_eval.model_overrides='{}'\", 'common_eval.results_path=null', 'distributed_training.distributed_world_size=128', 'distributed_training.distributed_rank=0', \"distributed_training.distributed_backend='nccl'\", \"distributed_training.distributed_init_method='tcp://learnfair0250:12597'\", 'distributed_training.distributed_port=12597', 'distributed_training.device_id=0', 'distributed_training.distributed_no_spawn=False', \"distributed_training.ddp_backend='c10d'\", 'distributed_training.bucket_cap_mb=25', 'distributed_training.fix_batches_to_gpus=False', 'distributed_training.find_unused_parameters=False', 'distributed_training.fast_stat_sync=False', 'distributed_training.heartbeat_timeout=-1', 'distributed_training.broadcast_buffers=False', \"distributed_training.distributed_wrapper='DDP'\", 'distributed_training.slowmo_momentum=null', \"distributed_training.slowmo_algorithm='LocalSGD'\", 'distributed_training.localsgd_frequency=3', 'distributed_training.nprocs_per_node=1', 'distributed_training.pipeline_model_parallel=False', 'distributed_training.pipeline_balance=null', 'distributed_training.pipeline_devices=null', 'distributed_training.pipeline_chunks=0', 'distributed_training.pipeline_encoder_balance=null', 'distributed_training.pipeline_encoder_devices=null', 'distributed_training.pipeline_decoder_balance=null', 'distributed_training.pipeline_decoder_devices=null', \"distributed_training.pipeline_checkpoint='never'\", \"distributed_training.zero_sharding='none'\", 'distributed_training.tpu=True', 'dataset.num_workers=1', 'dataset.skip_invalid_size_inputs_valid_test=False', 'dataset.max_tokens=3584', 'dataset.batch_size=null', 'dataset.required_batch_size_multiple=8', 'dataset.required_seq_len_multiple=1', 'dataset.dataset_impl=null', 'dataset.data_buffer_size=10', \"dataset.train_subset='train'\", \"dataset.valid_subset='valid'\", 'dataset.validate_interval=1', 'dataset.validate_interval_updates=0', 'dataset.validate_after_updates=0', 'dataset.fixed_validation_seed=null', 'dataset.disable_validation=False', \"dataset.max_tokens_valid='${dataset.max_tokens}'\", \"dataset.batch_size_valid='${dataset.batch_size}'\", 'dataset.curriculum=0', \"dataset.gen_subset='test'\", 'dataset.num_shards=1', 'dataset.shard_id=0', 'optimization.max_epoch=0', 'optimization.max_update=150000', 'optimization.stop_time_hours=0.0', 'optimization.clip_norm=0.0', 'optimization.sentence_avg=False', 'optimization.update_freq=[1]', 'optimization.lr=[0.0005]', 'optimization.stop_min_lr=1e-09', 'optimization.use_bmuf=False', \"checkpoint.save_dir='/checkpoint/edunov/20180526/wmt18en2de.wmt18.transformer_vaswani_wmt_en_de_big.bsz3584_lr0.0005_dr0.3_size225668919_sample_attdr0.1_upsample16'\", \"checkpoint.restore_file='checkpoint_last.pt'\", 'checkpoint.finetune_from_model=null', 'checkpoint.reset_dataloader=False', 'checkpoint.reset_lr_scheduler=False', 'checkpoint.reset_meters=False', 'checkpoint.reset_optimizer=False', \"checkpoint.optimizer_overrides='{}'\", 'checkpoint.save_interval=1', 'checkpoint.save_interval_updates=null', 'checkpoint.keep_interval_updates=0', 'checkpoint.keep_last_epochs=-1', 'checkpoint.keep_best_checkpoints=-1', 'checkpoint.no_save=False', 'checkpoint.no_epoch_checkpoints=False', 'checkpoint.no_last_checkpoints=False', 'checkpoint.no_save_optimizer_state=False', \"checkpoint.best_checkpoint_metric='loss'\", 'checkpoint.maximize_best_checkpoint_metric=False', 'checkpoint.patience=-1', \"checkpoint.checkpoint_suffix=''\", 'checkpoint.checkpoint_shard_count=1', 'checkpoint.load_checkpoint_on_all_dp_ranks=False', \"checkpoint.model_parallel_size='${common.model_parallel_size}'\", 'checkpoint.distributed_rank=0', 'bmuf.block_lr=1.0', 'bmuf.block_momentum=0.875', 'bmuf.global_sync_iter=50', 'bmuf.warmup_iterations=500', 'bmuf.use_nbm=False', 'bmuf.average_sync=False', 'bmuf.distributed_world_size=128', 'generation.beam=5', 'generation.nbest=1', 'generation.max_len_a=0.0', 'generation.max_len_b=200', 'generation.min_len=1', 'generation.match_source_len=False', 'generation.unnormalized=False', 'generation.no_early_stop=False', 'generation.no_beamable_mm=False', 'generation.lenpen=1.0', 'generation.unkpen=0.0', 'generation.replace_unk=null', 'generation.sacrebleu=False', 'generation.score_reference=False', 'generation.prefix_size=0', 'generation.no_repeat_ngram_size=0', 'generation.sampling=False', 'generation.sampling_topk=-1', 'generation.sampling_topp=-1.0', 'generation.constraints=null', 'generation.temperature=1.0', 'generation.diverse_beam_groups=-1', 'generation.diverse_beam_strength=0.5', 'generation.diversity_rate=-1.0', 'generation.print_alignment=null', 'generation.print_step=False', 'generation.lm_path=null', 'generation.lm_weight=0.0', 'generation.iter_decode_eos_penalty=0.0', 'generation.iter_decode_max_iter=10', 'generation.iter_decode_force_max_iter=False', 'generation.iter_decode_with_beam=1', 'generation.iter_decode_with_external_reranker=False', 'generation.retain_iter_history=False', 'generation.retain_dropout=False', 'generation.retain_dropout_modules=null', 'generation.decoding_format=null', 'generation.no_seed_provided=False', 'eval_lm.output_word_probs=False', 'eval_lm.output_word_stats=False', 'eval_lm.context_window=0', 'eval_lm.softmax_batch=9223372036854775807', 'interactive.buffer_size=0', \"interactive.input='-'\", 'tokenizer=moses', 'tokenizer._name=moses', \"tokenizer.source_lang='en'\", \"tokenizer.target_lang='de'\", 'tokenizer.moses_no_dash_splits=False', 'tokenizer.moses_no_escape=False', 'bpe=subword_nmt', 'bpe._name=subword_nmt', \"bpe.bpe_codes='/Users/luluo/.cache/torch/pytorch_fairseq/132307b1ed6b7c35ac7cc955bed54ec7fa87b612f07da9e5809fa49daedafb2d.09d4a0e2212ce4f65bf3baef404268c8ae0f4605f6f47246d489b600e0ed1b25/code'\", \"bpe.bpe_separator='@@'\", 'optimizer=adam', 'optimizer._name=adam', \"optimizer.adam_betas='(0.9, 0.98)'\", 'optimizer.adam_eps=1e-08', 'optimizer.weight_decay=0.0', 'optimizer.use_old_adam=False', 'optimizer.tpu=True', 'optimizer.lr=[0.0005]', 'lr_scheduler=inverse_sqrt', 'lr_scheduler._name=inverse_sqrt', 'lr_scheduler.warmup_updates=4000', 'lr_scheduler.warmup_init_lr=1e-07', 'lr_scheduler.lr=[0.0005]']\n"
     ]
    },
    {
     "ename": "ConfigCompositionException",
     "evalue": "Error merging override checkpoint.save_interval_updates=null",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\u001b[0m in \u001b[0;36m_apply_overrides_to_config\u001b[0;34m(overrides, cfg)\u001b[0m\n\u001b[1;32m    512\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                         \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConfigAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigKeyError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/omegaconf.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(cfg, key, value, merge)\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaConfBaseException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_and_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m__set_impl\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_and_normalize_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/basecontainer.py\u001b[0m in \u001b[0;36m_set_item_impl\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_validate_set\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_merge_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_assign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/dictconfig.py\u001b[0m in \u001b[0;36m_validate_set_merge_impl\u001b[0;34m(self, key, value, is_assign)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     self._format_and_raise(\n\u001b[0m\u001b[1;32m    149\u001b[0m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/base.py\u001b[0m in \u001b[0;36m_format_and_raise\u001b[0;34m(self, key, value, cause, type_override)\u001b[0m\n\u001b[1;32m     94\u001b[0m     ) -> None:\n\u001b[0;32m---> 95\u001b[0;31m         format_and_raise(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0m_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/omegaconf/_utils.py\u001b[0m in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m  \u001b[0;31m# set end OC_CAUSE=1 for full backtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: child 'checkpoint.save_interval_updates' is not Optional\n\tfull_key: checkpoint.save_interval_updates\n\treference_type=CheckpointConfig\n\tobject_type=CheckpointConfig",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConfigCompositionException\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7c2274010263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m en2de_ensemble = torch.hub.load(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'pytorch/fairseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformer.wmt18.en-de'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wmt18.model1.pt:wmt18.model2.pt:wmt18.model3.pt:wmt18.model4.pt:wmt18.model5.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     tokenizer='moses', bpe='subword_nmt')\n\u001b[1;32m      5\u001b[0m de2en_ensemble = torch.hub.load(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/models/fairseq_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhub_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         x = hub_utils.from_pretrained(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_user_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     models, args, task = checkpoint_utils.load_model_ensemble_and_task(\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpathsep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0marg_overrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_model_ensemble_and_task\u001b[0;34m(filenames, arg_overrides, task, strict, suffix, num_shards, state)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model file not found: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint_to_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_overrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"args\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_namespace_to_omegaconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/checkpoint_utils.py\u001b[0m in \u001b[0;36mload_checkpoint_to_cpu\u001b[0;34m(path, arg_overrides, load_on_all_ranks)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0moverwrite_args_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_overrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upgrade_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/checkpoint_utils.py\u001b[0m in \u001b[0;36m_upgrade_state_dict\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_namespace_to_omegaconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"cfg\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/dataclass/utils.py\u001b[0m in \u001b[0;36mconvert_namespace_to_omegaconf\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mcomposed_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error when composing. Overrides: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/experimental/compose.py\u001b[0m in \u001b[0;36mcompose\u001b[0;34m(config_name, overrides, strict, return_hydra_config)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalHydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydra\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     cfg = gh.hydra.compose_config(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/_internal/hydra.py\u001b[0m in \u001b[0;36mcompose_config\u001b[0;34m(self, config_name, overrides, run_mode, strict, with_log_configuration, from_shell)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_main_config_source_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         cfg = self.config_loader.load_configuration(\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\u001b[0m in \u001b[0;36mload_configuration\u001b[0;34m(self, config_name, overrides, run_mode, strict, from_shell)\u001b[0m\n\u001b[1;32m    149\u001b[0m     ) -> DictConfig:\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             return self._load_configuration(\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\u001b[0m in \u001b[0;36m_load_configuration\u001b[0;34m(self, config_name, overrides, run_mode, strict, from_shell)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# Apply command line overrides after enabling strict flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mConfigLoaderImpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_overrides_to_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_overrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mapp_overrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/hydra/_internal/config_loader_impl.py\u001b[0m in \u001b[0;36m_apply_overrides_to_config\u001b[0;34m(overrides, cfg)\u001b[0m\n\u001b[1;32m    518\u001b[0m                         ) from ex\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOmegaConfBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 raise ConfigCompositionException(\n\u001b[0m\u001b[1;32m    521\u001b[0m                     \u001b[0;34mf\"Error merging override {override.input_line}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 ) from ex\n",
      "\u001b[0;31mConfigCompositionException\u001b[0m: Error merging override checkpoint.save_interval_updates=null"
     ]
    }
   ],
   "source": [
    "en2de_ensemble = torch.hub.load(\n",
    "    'pytorch/fairseq', 'transformer.wmt18.en-de',\n",
    "    checkpoint_file='wmt18.model1.pt:wmt18.model2.pt:wmt18.model3.pt:wmt18.model4.pt:wmt18.model5.pt',\n",
    "    tokenizer='moses', bpe='subword_nmt')\n",
    "de2en_ensemble = torch.hub.load(\n",
    "    'pytorch/fairseq', 'transformer.wmt18.de-en',\n",
    "    checkpoint_file='wmt18.model1.pt:wmt18.model2.pt:wmt18.model3.pt:wmt18.model4.pt:wmt18.model5.pt',\n",
    "    tokenizer='moses', bpe='subword_nmt')\n",
    "\n",
    "en2de_ensemble.translate('Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "encoded_dict = tokenizer(text=text, add_special_tokens=True, padding='max_length', max_length=512, return_attention_mask=True, return_tensors='pt')\n",
    "input_ids = encoded_dict['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [50264] cannot be broadcast to indexing result of shape [412]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-16cebb5cb6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# This means leaving 10% unchanged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0minp_mask_2mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m input_ids_masked[\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0minp_mask_2mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m ] = mask_token_id  # mask token is the last in the dict\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [50264] cannot be broadcast to indexing result of shape [412]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# def get_masked_input_and_labels(input_ids):\n",
    "mask_token_id = torch.Tensor(50264)\n",
    "size = input_ids.size()\n",
    "# 15% BERT masking\n",
    "inp_mask = torch.randn(size) < 0.15\n",
    "# Do not mask special tokens\n",
    "inp_mask[input_ids<=2] = False\n",
    "# # Set targets to -1 by default, it means ignore\n",
    "# labels = -1 * np.ones(size, dtype=int)\n",
    "# # Set labels for masked tokens\n",
    "# labels[inp_mask] = input_ids[inp_mask]\n",
    "\n",
    "# Prepare input\n",
    "input_ids_masked = torch.clone(input_ids)\n",
    "# Set input to [MASK] which is the last token for the 90% of tokens\n",
    "# This means leaving 10% unchanged\n",
    "inp_mask_2mask = inp_mask & (torch.randn(size) < 0.90)\n",
    "input_ids_masked[inp_mask_2mask] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "# Set 10% to a random token\n",
    "inp_mask_2random = inp_mask_2mask & (torch.randn(size) < 1 / 9)\n",
    "input_ids_masked[inp_mask_2random] = torch.randint(\n",
    "    low=3, high=50262, size=torch.sum(inp_mask_2random)\n",
    ")\n",
    "\n",
    "# # Prepare sample_weights to pass to .fit() method\n",
    "# sample_weights = np.ones(labels.shape)\n",
    "# sample_weights[labels == -1] = 0\n",
    "\n",
    "# y_labels would be same as input_ids i.e input tokens\n",
    "y_labels = torch.clone(input_ids)\n",
    "\n",
    "#     return input_ids_masked, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 912])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"normal_kernel_cpu\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-de46f26caebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_id_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_masked_input_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-d8e49b743eaa>\u001b[0m in \u001b[0;36mget_masked_input_and_labels\u001b[0;34m(input_ids)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmask_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50264\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 15% BERT masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minp_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Do not mask special tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minp_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"normal_kernel_cpu\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "input_id_masked, y_labels = get_masked_input_and_labels(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      " 69%|██████▉   | 8223920128/11958904958 [3:48:23<1:43:43, 600135.83B/s]\n",
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      "100%|██████████| 11958904958/11958904958 [15:37<00:00, 12750993.89B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The speedy brown fox jumped over the lazy dog'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The quick brown fox jumped over the lazy dog'\n",
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='transformer.wmt19.en-de',\n",
    "    to_model_name='transformer.wmt19.de-en')\n",
    "back_translation_aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4480"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def back_translation(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, backtranslate the text column and append to the original dataframe with labels \n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing text and labels\n",
    "        col_name (str): Name of the text column\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing original text and backtranslated text and labels\n",
    "    \"\"\"\n",
    "\n",
    "    MODELS = {\n",
    "        'GERMAN': ('transformer.wmt19.en-de', 'transformer.wmt19.de-en'),\n",
    "        'RUSSIAN': ('transformer.wmt19.en-ru', 'transformer.wmt19.ru-en')\n",
    "    }\n",
    "\n",
    "    for _, model in MODELS.items():\n",
    "        back_translation_aug = naw.BackTranslationAug(\n",
    "            from_model_name=model[0],\n",
    "            to_model_name=model[1]\n",
    "        )\n",
    "        translation = df[col_name].apply(back_translation_aug.augment)\n",
    "        print(translation)\n",
    "        labels = df['class_id']\n",
    "        trans_df = pd.DataFrame({\n",
    "            'text': translation,\n",
    "            'class_id': labels\n",
    "        })\n",
    "        df = pd.concat([df, trans_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n",
      "Using cache found in /Users/luluo/.cache/torch/hub/pytorch_fairseq_master\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Size of sample #0 is invalid (=(1616, 0)) since max_positions=(1024, 1024), skip this example with --skip-invalid-size-inputs-valid-test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-68c79641fe1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-240a0583187b>\u001b[0m in \u001b[0;36mback_translation\u001b[0;34m(df, col_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mto_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_translation_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/base_augmenter.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(self, data, n, num_thread)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# https://discuss.pytorch.org/t/using-cuda-multiprocessing-with-single-gpu/7300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                         \u001b[0maugmented_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/augmenter/word/back_translation.py\u001b[0m in \u001b[0;36msubstitute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0maugmented_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nlpaug/model/lang_models/fairseq.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mback_translated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     ) -> List[str]:\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     def sample(\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     ) -> List[str]:\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mbatched_hypos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sentences, beam, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mbatched_hypos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhypos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatched_hypos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized_sentences, beam, verbose, skip_invalid_size_inputs, inference_step_args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0minference_step_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_step_args\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_invalid_size_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             translations = self.task.inference_step(\n",
      "\u001b[0;32m~/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36m_build_batches\u001b[0;34m(self, tokens, skip_invalid_size_inputs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     ) -> Iterator[Dict[str, Any]]:\n\u001b[1;32m    257\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         batch_iterator = self.task.get_batch_iterator(\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset_for_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36mget_batch_iterator\u001b[0;34m(self, dataset, max_tokens, max_sentences, max_positions, ignore_invalid_inputs, required_batch_size_multiple, seed, num_shards, shard_id, num_workers, epoch, data_buffer_size, disable_iterator_cache)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# filter examples that are too large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             indices = self.filter_indices_by_size(\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_invalid_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n",
      "\u001b[0;32m~/fairseq/fairseq/tasks/fairseq_task.py\u001b[0m in \u001b[0;36mfilter_indices_by_size\u001b[0;34m(self, indices, dataset, max_positions, ignore_invalid_inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignored\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_invalid_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 raise Exception(\n\u001b[0m\u001b[1;32m    150\u001b[0m                     (\n\u001b[1;32m    151\u001b[0m                         \u001b[0;34m\"Size of sample #{} is invalid (={}) since max_positions={}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Size of sample #0 is invalid (=(1616, 0)) since max_positions=(1024, 1024), skip this example with --skip-invalid-size-inputs-valid-test"
     ]
    }
   ],
   "source": [
    "with open('data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "df = back_translation(df, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV Split texts into segments to build more training data\n",
    "Because the overall lengths of the training texts are much longer than 510, here I experiment on creating more training data by dividing the documents into chunks of lengths between 300 and 510. I don't want anything too short that is hard to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['class_id'], test_size=0.2, random_state=2020)\n",
    "train_dataset = pd.DataFrame(list(zip(train_texts, train_labels)), columns=['text', 'class_id'])    \n",
    "test_dataset = pd.DataFrame(list(zip(test_texts, test_labels)), columns=['text', 'class_id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_bert import *\n",
    "split_train_df = split_training_data(train_dataset, 510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('split_train_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(split_train_df, handle)\n",
    "with open('test_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_dataset, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The  Friday  Cover  is  POLITICO  Magazine's  ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cracking down  on  religion.  Poland’s  fierce...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polish leaders  sometimes  talk  of  military ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Al-Qa‘ida  in  the  Arabian  Peninsula  is  a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Published  Nov.  16,  2020  4:54AM  ET  Photo ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>until “the  Mujahideen  has  irreversibly  com...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Stories  being  covered  today  by  BBC  Monit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>last month  after  negotiations  with  the  fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>With  communication  lines  cut  and  roads  b...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>Federico Soda,  IOM’s  Libya  chief  of  missi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>609 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    The  Friday  Cover  is  POLITICO  Magazine's  ...      3\n",
       "1    cracking down  on  religion.  Poland’s  fierce...      3\n",
       "2    Polish leaders  sometimes  talk  of  military ...      3\n",
       "3    Al-Qa‘ida  in  the  Arabian  Peninsula  is  a ...      2\n",
       "4    Published  Nov.  16,  2020  4:54AM  ET  Photo ...      3\n",
       "..                                                 ...    ...\n",
       "604  until “the  Mujahideen  has  irreversibly  com...      2\n",
       "605  Stories  being  covered  today  by  BBC  Monit...      1\n",
       "606  last month  after  negotiations  with  the  fe...      1\n",
       "607  With  communication  lines  cut  and  roads  b...      3\n",
       "608  Federico Soda,  IOM’s  Libya  chief  of  missi...      3\n",
       "\n",
       "[609 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
