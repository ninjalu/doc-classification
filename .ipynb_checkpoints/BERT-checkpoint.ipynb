{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with BERT\n",
    "Before diving into the details: the training and validation were not performed from this notebook but a AWS EC2 instance. If you want to replicate the result, please run BERT.py on a machine with GPU. </n>\n",
    "\n",
    "After playing around with traditional NLP methods, I think  them not being able to capturing the meaning of words and styles of writing may have a big impact on the accuracy. Transformers have a deeper understanding of words and sequences beyond frequencies. In attempt to improve accuracy, I propose the following strategies to try. Each may have variances within and trying to solve different challenges of BERT. </n>\n",
    "\n",
    "1. **BERT for sequence classification model with pretrained weights and embeddings:** </n>\n",
    "- Simple truncate: truncate the document head only, tail only, and head and tail; Hierachical method: divide the document into L/510 fragments and then use mean pooling, max pooling and self attention to combine hidden states of [CLS] for each fragment. There is [suggestion](https://arxiv.org/pdf/1905.05583.pdf) that head and tail produces better results on certain corpus than hierachical methods. </n>\n",
    "- [Longformer](https://arxiv.org/pdf/2004.05150.pdf): A transformer that handles long documents. [Code](https://github.com/allenai/longformer)\n",
    "\n",
    "2. **Fine tune BERT MLM on a domain specific corpus and use the updated weights (and embeddings) for BERT sequence classification**\n",
    "\n",
    "3. **Ensemble of traditional NLP with BERT.**\n",
    "\n",
    "4. **Knowledge distillation** \n",
    "5. **Make more data dividing documents into segments, each labelled accordingly.\n",
    "\n",
    "Option 1 and 4 is trying to address the problem of long sequence, 2 lack of domain specific training, and 3 tackling the problem from different angles (tranditional NLP: frequencies, BERT: word meaning and style). </n>\n",
    "\n",
    "Because of time and resource contraint we are going to try combining 1 with head truncation and 3. It's possible that traditional NLP and neural NLP could provide insights from different aspects and it would be interesting to explore here. Given time and resouce, options 2 and 4 would be very interesting to explore too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import BertEncoder, build_dataloaders\n",
    "from fine_tune_bert import fine_tune_bert\n",
    "import pickle5 as pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    label = df['class_id'].apply(lambda x: int(x))\n",
    "    \n",
    "    texts, _, label, _ = train_test_split(texts, label, test_size=0.2, stratify=label, random_state=2020)\n",
    "    dataset = BertEncoder(\n",
    "        tokenizer=BertTokenizer.from_pretrained(\n",
    "            'bert-base-cased', \n",
    "            do_lower_case=False), \n",
    "        input_data=texts\n",
    "    )\n",
    "\n",
    "    data = dataset.tokenize(max_len=510)\n",
    "    input_ids, attention_masks = data\n",
    "    label = torch.Tensor(label.to_list()).long()\n",
    "    train_dataloader, val_dataloader = build_dataloaders(\n",
    "        input_ids=input_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        labels=label,\n",
    "        batch_size=(16, 16),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    trained_model, stats = fine_tune_bert(\n",
    "        train_dataloader=train_dataloader, \n",
    "        valid_dataloader=val_dataloader, \n",
    "        model=bert_model,\n",
    "        optimizer=optimizer,\n",
    "        save_model_path='model/trained_model.pt',\n",
    "        save_stats_dict_path='logs/statistics.json',\n",
    "        device = device,\n",
    "        epochs = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 21 epochs of training, before it was stopped because validation accuracy had not changed for a few epochs: </n>\n",
    "![Screenshot](training_result.png) </n>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is saved and ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.77      0.94      0.85        18\n",
      " Situation Report       0.94      0.85      0.89        20\n",
      "   Profile report       1.00      0.65      0.79        20\n",
      "Analytical report       0.75      1.00      0.86        15\n",
      "\n",
      "         accuracy                           0.85        73\n",
      "        macro avg       0.87      0.86      0.85        73\n",
      "     weighted avg       0.88      0.85      0.85        73\n",
      "\n",
      "[[17  1  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 4  0 13  3]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('tokenised-data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=BertTokenizer.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        do_lower_case=False),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "bert_state_dict = torch.load('./model/bfs_trained_model.pt')\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    state_dict=bert_state_dict,\n",
    "    num_labels = 4,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "y_b_pred, _ = bert_predict(bert, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is one drawback of vanilla BERT model: the tokeniser and pre-trained BERT model were trained on Wikipedia and BookCorpus. Although it might have a good 'understanding' of Wiki and book English, it has not seen news and analytical reports that are closer to our training data. </n>\n",
    "Here enters [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). The advantage of RoBERTa here are two. First, it was pre-trained on Wikipedia, BookCorpus, plus CC-NEWS, OpenWebText and STORIES, the later three are all texts from the internet, much closer to the kind of texts we see in our corpus. Two, RoBERTa also employs different training strategies, arguably more effective for downstream tasks. </n>\n",
    "However, we managed to achieve no loss in the first epoch (not sure it is a good thing or not?). Either out training data is too small, or RoBERTa is too great?</n>\n",
    "I evaluate both BERT model and RoBERTa model using the same testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenised-data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_train.to_csv('data_train.csv')\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_test.to_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
