{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with BERT\n",
    "Before diving into the details: the training and validation were not performed from this notebook but a AWS EC2 instance. If you want to replicate the result, please run BERT.py on a machine with GPU. </n>\n",
    "\n",
    "After playing around with traditional NLP methods, I think  them not being able to capturing the meaning of words and styles of writing may have a big impact on the accuracy. Transformers have a deeper understanding of words and sequences beyond frequencies. In attempt to improve accuracy, I propose the following strategies to try. Each may have variances within and trying to solve different challenges of BERT. </n>\n",
    "\n",
    "1. **BERT for sequence classification model with pretrained weights and embeddings:** </n>\n",
    "- Simple truncate: truncate the document head only, tail only, and head and tail; Hierachical method: divide the document into L/510 fragments and then use mean pooling, max pooling and self attention to combine hidden states of [CLS] for each fragment. There is [suggestion](https://arxiv.org/pdf/1905.05583.pdf) that head and tail produces better results on certain corpus than hierachical methods. </n>\n",
    "- [Longformer](https://arxiv.org/pdf/2004.05150.pdf): A transformer that handles long documents. [Code](https://github.com/allenai/longformer)\n",
    "\n",
    "2. **Fine tune BERT MLM on a domain specific corpus and use the updated weights (and embeddings) for BERT sequence classification**\n",
    "\n",
    "3. **Ensemble of traditional NLP with BERT.**\n",
    "\n",
    "4. **Knowledge distillation** \n",
    "5. **Make more data dividing documents into segments, each labelled accordingly.\n",
    "\n",
    "Option 1 and 4 is trying to address the problem of long sequence, 2 lack of domain specific training, and 3 tackling the problem from different angles (tranditional NLP: frequencies, BERT: word meaning and style). </n>\n",
    "\n",
    "Because of time and resource contraint we are going to try combining 1 with head truncation and 3. It's possible that traditional NLP and neural NLP could provide insights from different aspects and it would be interesting to explore here. Given time and resouce, options 2 and 4 would be very interesting to explore too!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import BertEncoder, build_dataloaders\n",
    "from fine_tune_bert import fine_tune_bert\n",
    "import pickle5 as pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    label = df['class_id'].apply(lambda x: int(x))\n",
    "    \n",
    "    texts, _, label, _ = train_test_split(texts, label, test_size=0.2, stratify=label, random_state=2020)\n",
    "    dataset = BertEncoder(\n",
    "        tokenizer=BertTokenizer.from_pretrained(\n",
    "            'bert-base-cased', \n",
    "            do_lower_case=False), \n",
    "        input_data=texts\n",
    "    )\n",
    "\n",
    "    data = dataset.tokenize(max_len=510)\n",
    "    input_ids, attention_masks = data\n",
    "    label = torch.Tensor(label.to_list()).long()\n",
    "    train_dataloader, val_dataloader = build_dataloaders(\n",
    "        input_ids=input_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        labels=label,\n",
    "        batch_size=(16, 16),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    trained_model, stats = fine_tune_bert(\n",
    "        train_dataloader=train_dataloader, \n",
    "        valid_dataloader=val_dataloader, \n",
    "        model=bert_model,\n",
    "        optimizer=optimizer,\n",
    "        save_model_path='model/trained_model.pt',\n",
    "        save_stats_dict_path='logs/statistics.json',\n",
    "        device = device,\n",
    "        epochs = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 21 epochs of training, before it was stopped because validation accuracy had not changed for a few epochs: </n>\n",
    "![Screenshot](training_result.png) </n>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is saved and ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-829f4be67d3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0my_b_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mcls_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Incident Report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Situation Report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Profile report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Analytical report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_predict' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from ensemble import \n",
    "\n",
    "with open('tokenised-data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "bert_state_dict = torch.load('./model/trained_model.pt')\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    state_dict=bert_state_dict,\n",
    "    num_labels = 4,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "y_b_pred, _ = bert_predict(bert, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
