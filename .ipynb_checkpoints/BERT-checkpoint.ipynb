{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with BERT\n",
    "Before diving into the details: the training and validation were not performed from this notebook but a AWS EC2 instance. If you want to replicate the result, please run BERT.py on a machine with GPU. </n>\n",
    "\n",
    "After playing around with traditional NLP methods, I think  them not being able to capturing the meaning of words and styles of writing may have a big impact on the accuracy. Transformers have a deeper understanding of words and sequences beyond frequencies. In attempt to improve accuracy, I propose the following strategies to try. Each may have variances within and trying to solve different challenges of BERT. </n>\n",
    "\n",
    "1. **BERT for sequence classification model with pretrained weights and embeddings:** </n>\n",
    "- Simple truncate: truncate the document head only, tail only, and head and tail; Hierachical method: divide the document into L/510 fragments and then use mean pooling, max pooling and self attention to combine hidden states of [CLS] for each fragment. There is [suggestion](https://arxiv.org/pdf/1905.05583.pdf) that head and tail produces better results on certain corpus than hierachical methods. </n>\n",
    "- [Longformer](https://arxiv.org/pdf/2004.05150.pdf): A transformer that handles long documents. [Code](https://github.com/allenai/longformer)\n",
    "\n",
    "2. **Fine tune BERT MLM on a domain specific corpus and use the updated weights (and embeddings) for BERT sequence classification**\n",
    "\n",
    "3. **Ensemble of traditional NLP with BERT.**\n",
    "\n",
    "4. **Knowledge distillation** \n",
    "5. **Make more data dividing documents into segments, each labelled accordingly.**\n",
    "6. **Back translation to create more data** (unfinished)\n",
    "\n",
    "Options 1 and 4, 5 are trying to address the problem of long sequence, 2 lack of domain specific training, and 3 tackling the problem from different angles (tranditional NLP: frequencies, BERT: word meaning and style). Option 6 is trying to tackle the problem of small training set. </n>\n",
    "\n",
    "Because of time and resource contraint I attempted combining 1 with head truncation, 2 and 6 with varied sucess. It's possible that traditional NLP and neural NLP could provide insights from different aspects and it would be interesting to explore here. Given time and resouce, other ideas would be good to explore too!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.a BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import BertEncoder, build_dataloaders\n",
    "from fine_tune_bert_sc import fine_tune_bert\n",
    "import pickle5 as pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('./data/data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    label = df['class_id'].apply(lambda x: int(x))\n",
    "    \n",
    "    texts, _, label, _ = train_test_split(texts, label, test_size=0.2, stratify=label, random_state=2020)\n",
    "    dataset = BertEncoder(\n",
    "        tokenizer=BertTokenizer.from_pretrained(\n",
    "            'bert-base-cased', \n",
    "            do_lower_case=False), \n",
    "        input_data=texts\n",
    "    )\n",
    "\n",
    "    data = dataset.tokenize(max_len=510)\n",
    "    input_ids, attention_masks = data\n",
    "    label = torch.Tensor(label.to_list()).long()\n",
    "    train_dataloader, val_dataloader = build_dataloaders(\n",
    "        input_ids=input_ids,\n",
    "        attention_masks=attention_masks,\n",
    "        labels=label,\n",
    "        batch_size=(16, 16),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    trained_model, stats = fine_tune_bert(\n",
    "        train_dataloader=train_dataloader, \n",
    "        valid_dataloader=val_dataloader, \n",
    "        model=bert_model,\n",
    "        optimizer=optimizer,\n",
    "        save_model_path='model/trained_model.pt',\n",
    "        save_stats_dict_path='logs/statistics.json',\n",
    "        device = device,\n",
    "        epochs = 30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from 21 epochs of training, before it was stopped because validation accuracy had not changed for a few epochs: </n>\n",
    "![Screenshot](images/training_result.png) </n>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is saved and ready for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.92      0.97      0.94        88\n",
      " Situation Report       0.97      0.89      0.93        99\n",
      "   Profile report       0.97      0.97      0.97       100\n",
      "Analytical report       0.91      0.96      0.93        74\n",
      "\n",
      "         accuracy                           0.94       361\n",
      "        macro avg       0.94      0.95      0.94       361\n",
      "     weighted avg       0.95      0.94      0.94       361\n",
      "\n",
      "[[85  2  0  1]\n",
      " [ 7 88  1  3]\n",
      " [ 0  0 97  3]\n",
      " [ 0  1  2 71]]\n"
     ]
    }
   ],
   "source": [
    "import pickle5 as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from ensemble import bert_predict\n",
    "from prep_bert import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "with open('./data/data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "texts = df['text'].tolist()\n",
    "labels = df['class_id'].apply(lambda x: int(x))\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=2020)\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=BertTokenizer.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        do_lower_case=False),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "bert_state_dict = torch.load('./model/bfs_trained_model.pt')\n",
    "bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    state_dict=bert_state_dict,\n",
    "    num_labels = 4,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "y_b_pred, _ = bert_predict(bert, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.b RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drawback of vanilla BERT model: the tokeniser and pre-trained BERT model were trained on Wikipedia and BookCorpus. Although it might have a good 'understanding' of Wiki and book English, it has not seen news and analytical reports that are closer to our training data. </n>\n",
    "Here enters [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf). The advantage of RoBERTa here are two. First, it was pre-trained on Wikipedia, BookCorpus, plus CC-NEWS, OpenWebText and STORIES, the later three are all texts from the internet, much closer to the kind of texts we see in our corpus. Two, RoBERTa also employs different training strategies, arguably more effective for downstream tasks. </n>\n",
    "However, we managed to achieve no loss in the first epoch (not sure it is a good thing or not?). Either out training data is too small, or RoBERTa is too great?</n>\n",
    "I evaluate both BERT model and RoBERTa model using the same testset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not run on CPU.\n",
    "\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prep_bert import *\n",
    "from fine_tune_bert_sc import *\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('./data/data.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['class_id'].apply(lambda x: int(x))\n",
    "    train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=2020)\n",
    "\n",
    "    train_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=train_texts\n",
    "    )\n",
    "    test_dataset = BertEncoder(\n",
    "        tokenizer=RobertaTokenizer.from_pretrained('roberta-base'), \n",
    "        input_data=test_texts\n",
    "    )\n",
    "\n",
    "    train_data = train_dataset.tokenize(max_len=510)\n",
    "    test_data = test_dataset.tokenize(max_len=510)\n",
    "\n",
    "    train_input_ids, train_attention_masks = train_data\n",
    "    test_input_ids, test_attention_masks = test_data\n",
    "\n",
    "    train_labels = torch.Tensor(train_labels.to_list()).long()\n",
    "    test_labels = torch.Tensor(test_labels.to_list()).long()\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        'roberta-base',\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        num_labels = 4\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='.results/',\n",
    "        num_train_epochs=8,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=dummy_data_collector\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model('.model/')\n",
    "    trainer.evaluate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.95      1.00      0.97        18\n",
      " Situation Report       1.00      0.85      0.92        20\n",
      "   Profile report       1.00      1.00      1.00        20\n",
      "Analytical report       0.88      1.00      0.94        15\n",
      "\n",
      "         accuracy                           0.96        73\n",
      "        macro avg       0.96      0.96      0.96        73\n",
      "     weighted avg       0.96      0.96      0.96        73\n",
      "\n",
      "[[18  0  0  0]\n",
      " [ 1 17  0  2]\n",
      " [ 0  0 20  0]\n",
      " [ 0  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation on test set\n",
    "\n",
    "with open('./data/data.pickle', 'rb') as handle:\n",
    "    df = pickle.load(handle)\n",
    "    \n",
    "X = df['text']\n",
    "y = df['class_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify=y, random_state=2020)\n",
    "texts = X_test.to_list()\n",
    "labels = y_test.apply(lambda x:int(x))\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=texts\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(labels.to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "roberta = RobertaForSequenceClassification.from_pretrained(\n",
    "    'model/roberta',\n",
    "    local_files_only=True\n",
    ")\n",
    "y_b_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_b_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It look like RoBERTa works a lot better than vanilla BERT model. Notice the model only misclassified 3 situation reports in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "  Incident Report       0.97      1.00      0.98        88\n",
      " Situation Report       1.00      0.95      0.97        99\n",
      "   Profile report       1.00      1.00      1.00       100\n",
      "Analytical report       0.96      0.99      0.97        74\n",
      "\n",
      "         accuracy                           0.98       361\n",
      "        macro avg       0.98      0.98      0.98       361\n",
      "     weighted avg       0.98      0.98      0.98       361\n",
      "\n",
      "[[ 88   0   0   0]\n",
      " [  2  94   0   3]\n",
      " [  0   0 100   0]\n",
      " [  1   0   0  73]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on all dataset\n",
    "\n",
    "dataset = BertEncoder(\n",
    "    tokenizer=RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    input_data=X.to_list()\n",
    ")\n",
    "\n",
    "tokenized_data = dataset.tokenize(max_len=510)\n",
    "\n",
    "input_ids, attention_masks = tokenized_data\n",
    "labels = torch.Tensor(y.apply(lambda x:int(x)).to_list()).long()\n",
    "\n",
    "dataloader = build_test_dataloaders(\n",
    "    input_ids=input_ids,\n",
    "    attention_masks=attention_masks,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "y_pred, _ = bert_predict(roberta, dataloader)\n",
    "cls_names = ['Incident Report', 'Situation Report', 'Profile report', 'Analytical report']\n",
    "print(classification_report(labels, y_pred, target_names=cls_names))\n",
    "print(confusion_matrix(labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = np.array(y_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model misclassified ground truth 26     1\n",
      "68     1\n",
      "75     1\n",
      "77     1\n",
      "336    1\n",
      "347    3\n",
      "Name: class_id, dtype: int64\n",
      "BERT model misclassified predicted [0 0 3 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "print('BERT model misclassified ground truth', y[y!=y_pred])\n",
    "print('BERT model misclassified predicted', np.array(y_pred)[y!=y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified cases are: </n>\n",
    "\n",
    "26 and 68 ground truth: situation report, prediction: incident report </n>\n",
    "\n",
    "75, 77 and 336 ground truth: situation report, prediction: profile report </n>\n",
    "\n",
    "347 ground truth: profile report, prediction: incident report </n>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Finetune the BERT model with a MLM task and finetune again with classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind this is the general idea pretrained models improve performance on downstream task. Because BERT might not have seen the kind of training data here, fine tuning BERT weights with the texts similar to the training data would be benefitial. </n>\n",
    "\n",
    "Because labelled data is hard to come by, BERT Masked Language Model would be very useful. </n>\n",
    "\n",
    "The original proposal is to obtain more documents of the similar kind, however, we weren't able to because of the time constraint. I could fine tune BERT with our training data on hand, however, I have little confidence it would work well because: first, less than 400 data points is too small to fine tune BERT weights effectively; second, if the model has seen the training data, would it constitute data leakage? </n>\n",
    "\n",
    "Maybe this is a good avenue to explore given additional data (how much?) </n>\n",
    "\n",
    "Given how well RoBERTa performs, I think it's fair to say any improvement on it with further MLM fine tuning would be quite amazing and proves how adaptable BERT is. The idea is not only for this classification task, but all downstream tasks such as entity recognition. It is probable that semi-supervised learning techniques would bring overall improvements on all tasks using similar kind of data. </n>\n",
    "\n",
    "This has a very exciting practical implication. Instead of updating the model with labelled data (expensive and not easy to get), update BERT weights with all new data as it comes in. Like making a master sour dough, the BERT weights will get better and better and evolve with change (new vocabulary, changes in document types) </n>\n",
    "\n",
    "Traditional NLP although also very effective and much quicker, will have to rely on retraining on labelled data to maintain. In addition, it will not be able to help down stream tasks such as entity recognition, sentiment analysis etc. Finally, even though training and inference takes very little time, particularly for randam forest, the time to tokenize is not particularly short if you have a large amount of data. Although, NN NLP has the same problem as well. </n>\n",
    "\n",
    "If in the future this method comes into use, refer to this: https://novetta.github.io/adaptnlp/tutorial/fine-tuning-language-model.html for easy fine tuning\n",
    "\n",
    "In summary, as the original [RoBERTa paper](https://arxiv.org/pdf/1907.11692.pdf) pointed out, it achieved state-of-the-art performance from bigger batches, longer sequences, longer training time and more data! I think it points to the fact that ceiling probably has not been achieved and further training and fine tuning will give better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
